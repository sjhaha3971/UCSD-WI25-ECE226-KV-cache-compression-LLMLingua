{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb26350-3e60-4fcc-97fa-483663a798b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stp 1 Dependecis instal\n",
    "#!pip install transformers==4.41.2\n",
    "# Important to install that transformer version, unles it occur error\n",
    "#See link: https://github.com/THUDM/CogVLM2/issues/181#issuecomment-2381807778\n",
    "# !pip install transformers sentencepiece datasets evaluate\n",
    "# !pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92bc4697-f92f-4838-92ab-a9dd306dd236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "#from LLMLingua.experiments.llmlingua2.evaluation.metrics import qa_f1_score\n",
    "from metrics import qa_f1_score, f1_score\n",
    "if 'model' in globals():\n",
    "    del model \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#######################################\n",
    "print('cuda is',torch.cuda.is_available()) \n",
    "#################################################\n",
    "# 3 model & dataset \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bcaafff-a6de-4b15-b4ef-be30536c8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "print('cuda is',torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41da0400-1574-4695-8eb0-4a8d2910d847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8ced9b160e48fc90b813777e224ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ChatGLM-6B\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"THUDM/chatglm2-6b-32k\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True\n",
    ").to(device)\n",
    "\n",
    "model=model.eval()\n",
    "#print('model config is:',model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b03872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b-32k\", trust_remote_code=True)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36cd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch the tokenizer's _pad method to handle the padding_side argument\n",
    "original_pad = tokenizer._pad\n",
    "def new_pad(self, encoded_inputs, max_length=None, padding_strategy=\"longest\", pad_to_multiple_of=None, return_attention_mask=None, **kwargs):\n",
    "    return original_pad(encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)\n",
    "tokenizer._pad = new_pad.__get__(tokenizer, type(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e2ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  four datasets\n",
    "datasets = [\"multifieldqa_en\", \"hotpotqa\", \"triviaqa\", \"narrativeqa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e2f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongjae/miniconda3/envs/seongjae_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LINGUA COMPRESSION\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "compressor = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "    device_map=\"cuda\",  # Use GPU\n",
    "    use_llmlingua2=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multifield(data, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    memory_usages = []\n",
    "    peak_memory_usages = []\n",
    "\n",
    "    #for idx in range(20): \n",
    "    for idx in range(len(data)):  # Test on all examples\n",
    "        example = data[idx] \n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers =example[\"answers\"][0]\n",
    "        compressed_context = compressor.compress_prompt(\n",
    "            context,\n",
    "            rate=ratio,           # Keep 50% tokens (compression ratio)\n",
    "        )[\"compressed_prompt\"]\n",
    "        prompt = f\"Read the following text and answer briefly.\\n\\n{compressed_context}\\n\\nNow, answer the following question based on the above text, only give me the answer and do not output any other words.\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_time = time.time()\n",
    "        response, _ = model.chat(tokenizer, prompt)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Measure memory\n",
    "        memory = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "        \n",
    "        #Measure peak memory?\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = qa_f1_score(prediction=response, ground_truth=answers)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "        peak_memory_usages.append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores),\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hotpot(data, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    memory_usages = []\n",
    "    peak_memory_usages = []\n",
    "\n",
    "    #for idx in range(20): \n",
    "    for idx in range(len(data)):  # Test on all examples\n",
    "        example = data[idx] \n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers =example[\"answers\"][0]\n",
    "        compressed_context = compressor.compress_prompt(\n",
    "            context,\n",
    "            rate=ratio,           # Keep 50% tokens (compression ratio)\n",
    "        )[\"compressed_prompt\"]\n",
    "        prompt = f\"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{compressed_context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_time = time.time()\n",
    "        response, _ = model.chat(tokenizer, prompt)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Measure memory\n",
    "        memory = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "        \n",
    "        #Measure peak memory?\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = qa_f1_score(prediction=response, ground_truth=answers)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "        peak_memory_usages.append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores),\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c80167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trivia(data, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    memory_usages = []\n",
    "    peak_memory_usages = []\n",
    "\n",
    "    #for idx in range(20): \n",
    "    for idx in range(len(data)):  # Test on all examples\n",
    "        example = data[idx] \n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers =example[\"answers\"][0]\n",
    "        compressed_context = compressor.compress_prompt(\n",
    "            context,\n",
    "            rate=ratio,           # Keep 50% tokens (compression ratio)\n",
    "        )[\"compressed_prompt\"]\n",
    "        prompt = f\"Answer the question based on the given passage. Only give me the answer and do not output any other words. The following are some examples.\\n\\n{compressed_context}\\n\\n{question}\"\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_time = time.time()\n",
    "        response, _ = model.chat(tokenizer, prompt)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Measure memory\n",
    "        memory = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "        \n",
    "        #Measure peak memory?\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = qa_f1_score(prediction=response, ground_truth=answers)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "        peak_memory_usages.append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores),\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64c8b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_narrative(data, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    peak_memory_usages = []\n",
    "    \n",
    "    # Model constraints\n",
    "    max_context_length = 32768  # ChatGLM2-6B-32k's actual limit\n",
    "    max_new_tokens = 256        # Reserve space for generation\n",
    "    max_input_length = max_context_length - max_new_tokens  # 32512\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        example = data[idx] \n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers = example[\"answers\"][0]\n",
    "        \n",
    "        # Convert answer to string if needed\n",
    "        if isinstance(answers, list):\n",
    "            answers = \" \".join(answers)\n",
    "        answers = str(answers)\n",
    "\n",
    "        try:\n",
    "            # 1. Compress context first\n",
    "            if ratio is not None:\n",
    "                compressed_context = compressor.compress_prompt(\n",
    "                    context,\n",
    "                    rate=ratio,\n",
    "                    target_token=int(max_input_length * 0.9)  # Leave room for prompt template\n",
    "                )[\"compressed_prompt\"]\n",
    "            else:\n",
    "                compressed_context = context  # No compression\n",
    "\n",
    "            # 2. Build full prompt\n",
    "            prompt = f\"You are given a story...\\n\\nStory: {compressed_context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "            \n",
    "            # 3. Tokenize and truncate the ENTIRE prompt\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_input_length\n",
    "            ).to(model.device)\n",
    "\n",
    "            # 4. Generate with length control\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.01\n",
    "            )\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # 5. Calculate metrics\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "            f1 = qa_f1_score(response.split(\"Answer:\")[-1].strip(), answers)\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "            latencies.append(latency)\n",
    "            peak_memory_usages.append(peak_memory)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores) if f1_scores else 0,\n",
    "        \"avg_latency\": np.mean(latencies) if latencies else 0,\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages) if peak_memory_usages else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f58e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable function to print and save results\n",
    "        \n",
    "def log_result_LLMLingua(message):\n",
    "    #print(message)\n",
    "    with open(\"results_LLMLingua.txt\", \"a\") as f:  # 'a' = append mode\n",
    "        f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22776e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===multifieldqa_en  llmlingua compression (25%) ===\n",
      "F1: 0.19, Latency: 2.47s,  Peak Memory: 14417.95 MB\n",
      "\n",
      "===multifieldqa_en  llmlingua compression (50%) ===\n",
      "F1: 0.27, Latency: 17.86s,  Peak Memory: 14752.85 MB\n",
      "\n",
      "===multifieldqa_en  llmlingua compression (75%) ===\n",
      "F1: 0.38, Latency: 1.41s,  Peak Memory: 15072.73 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[0]  #multifiledqa_en\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate_multifield(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a98b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===hotpotqa  llmlingua compression (25%) ===\n",
      "F1: 0.30, Latency: 0.70s,  Peak Memory: 14749.06 MB\n",
      "\n",
      "===hotpotqa  llmlingua compression (50%) ===\n",
      "F1: 0.35, Latency: 1.25s,  Peak Memory: 15346.20 MB\n",
      "\n",
      "===hotpotqa  llmlingua compression (75%) ===\n",
      "F1: 0.38, Latency: 1.92s,  Peak Memory: 15934.34 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[1]  #hotpotqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate_hotpot(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6556788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===triviaqa  llmlingua compression (25%) ===\n",
      "F1: 0.42, Latency: 0.70s,  Peak Memory: 14769.61 MB\n",
      "\n",
      "===triviaqa  llmlingua compression (50%) ===\n",
      "F1: 0.40, Latency: 1.20s,  Peak Memory: 15281.46 MB\n",
      "\n",
      "===triviaqa  llmlingua compression (75%) ===\n",
      "F1: 0.41, Latency: 1.78s,  Peak Memory: 15789.83 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[2]  #triviaqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate_trivia(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "917a8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongjae/miniconda3/envs/seongjae_env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===narrativeqa  llmlingua compression (25%) ===\n",
      "F1: 0.21, Latency: 9.47s,  Peak Memory: 17819.50 MB\n",
      "\n",
      "===narrativeqa  llmlingua compression (50%) ===\n",
      "F1: 0.21, Latency: 9.68s,  Peak Memory: 17819.39 MB\n",
      "\n",
      "===narrativeqa  llmlingua compression (75%) ===\n",
      "F1: 0.21, Latency: 9.04s,  Peak Memory: 17819.39 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[3]  #narrativeqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate_narrative(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a2b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seongjae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
